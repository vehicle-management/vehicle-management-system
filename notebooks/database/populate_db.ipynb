{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in /opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages (9.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Database Population\n",
    "## populates all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mysql.connector import connect, Error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "conn = None\n",
    "\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        global conn\n",
    "        conn = connect(\n",
    "            host='localhost',\n",
    "            user='root',\n",
    "            password='password',\n",
    "            database='final'\n",
    "        )\n",
    "        print(\"Connection successful!\")\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/cleaned/enterprise_station.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m used_cars_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/cleaned/used_cars_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/cleaned/enterprise_station.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m enterprise_station_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m car_dekho_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(car_dekho_path)\n\u001b[1;32m      8\u001b[0m clean_car_sales_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(clean_car_sales_data_path)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-vehicle-management-system/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/cleaned/enterprise_station.csv'"
     ]
    }
   ],
   "source": [
    "car_dekho_path = '../../data/cleaned/car_dekho.csv'\n",
    "clean_car_sales_data_path = '../../data/cleaned/clean_car_sales_data.csv'\n",
    "used_cars_data_path = '../../data/cleaned/used_cars_data.csv'\n",
    "data_path = '../../data/cleaned/enterprise_station.csv'\n",
    "\n",
    "enterprise_station_df = pd.read_csv(data_path)\n",
    "car_dekho_data = pd.read_csv(car_dekho_path)\n",
    "clean_car_sales_data = pd.read_csv(clean_car_sales_data_path)\n",
    "used_cars_data = pd.read_csv(used_cars_data_path)\n",
    "\n",
    "car_rental_path = '../../data/cleaned/car_rental_sample.csv'\n",
    "clean_car_sales_data_path = '../../data/cleaned/clean_car_sales_data.csv'\n",
    "\n",
    "rental_data = pd.read_csv(car_rental_path)\n",
    "clean_car_sales_data = pd.read_csv(clean_car_sales_data_path)\n",
    "\n",
    "sales_data_path = '../../data/cleaned/clean_car_sales_data.csv'\n",
    "sales_data = pd.read_csv(sales_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create / Reset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Affected 11 rows.\n",
      "Affected 1 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Affected 0 rows.\n",
      "Schema created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_schema(conn):\n",
    "    \"\"\"\n",
    "    Create the 'final' schema and all required tables.\n",
    "    \"\"\"\n",
    "    schema_sql = \"\"\"\n",
    "    DROP SCHEMA IF EXISTS final;\n",
    "    CREATE SCHEMA final;\n",
    "    USE final;\n",
    "\n",
    "    CREATE TABLE make (\n",
    "        make_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        make_name VARCHAR(255)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE vehicle (\n",
    "        vehicle_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        make_id INT,\n",
    "        model VARCHAR(255),\n",
    "        year INT,\n",
    "        price INT,\n",
    "        commission_rate DOUBLE,\n",
    "        FOREIGN KEY (make_id) REFERENCES make(make_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE store (\n",
    "        store_id INT PRIMARY KEY,\n",
    "        brand VARCHAR(255),\n",
    "        location_name VARCHAR(255),\n",
    "        location_number VARCHAR(255),\n",
    "        location_type VARCHAR(255),\n",
    "        address VARCHAR(255),\n",
    "        country VARCHAR(255),\n",
    "        city VARCHAR(255),\n",
    "        state VARCHAR(255),\n",
    "        postal_code VARCHAR(255),\n",
    "        latitude DOUBLE,\n",
    "        longitude DOUBLE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE staff (\n",
    "        staff_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        name VARCHAR(255),\n",
    "        store_id INT,\n",
    "        hire_date DATE,\n",
    "        FOREIGN KEY (store_id) REFERENCES store(store_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE client (\n",
    "        client_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        name VARCHAR(255),\n",
    "        salary INT,\n",
    "        purchased TINYINT(1)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE rental (\n",
    "        rental_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        vehicle_id INT,\n",
    "        client_id INT,\n",
    "        rental_rate INT,\n",
    "        rental_length INT,\n",
    "        start_date DATE,\n",
    "        start_time TIME,\n",
    "        return_date DATE,\n",
    "        return_time TIME,\n",
    "        FOREIGN KEY (vehicle_id) REFERENCES vehicle(vehicle_id),\n",
    "        FOREIGN KEY (client_id) REFERENCES client(client_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE sale (\n",
    "        sale_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        vehicle_id INT,\n",
    "        client_id INT,\n",
    "        staff_id INT,\n",
    "        sale_price INT,\n",
    "        commission_rate DOUBLE,\n",
    "        sale_date DATE,\n",
    "        FOREIGN KEY (vehicle_id) REFERENCES vehicle(vehicle_id),\n",
    "        FOREIGN KEY (client_id) REFERENCES client(client_id),\n",
    "        FOREIGN KEY (staff_id) REFERENCES staff(staff_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE commission (\n",
    "        commission_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        staff_id INT,\n",
    "        sale_id INT,\n",
    "        amount DOUBLE,\n",
    "        FOREIGN KEY (staff_id) REFERENCES staff(staff_id),\n",
    "        FOREIGN KEY (sale_id) REFERENCES sale(sale_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE vehicle_performance (\n",
    "        performance_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        vehicle_id INT,\n",
    "        rating DOUBLE,\n",
    "        trips_taken INT,\n",
    "        review_count INT,\n",
    "        FOREIGN KEY (vehicle_id) REFERENCES vehicle(vehicle_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE transaction_history (\n",
    "        transaction_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        rental_id INT,\n",
    "        store_id INT,\n",
    "        payment INT,\n",
    "        FOREIGN KEY (rental_id) REFERENCES rental(rental_id),\n",
    "        FOREIGN KEY (store_id) REFERENCES store(store_id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE client_account (\n",
    "        account_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        client_id INT,\n",
    "        balance DECIMAL(10, 2),\n",
    "        created_date DATE,\n",
    "        status VARCHAR(255),\n",
    "        FOREIGN KEY (client_id) REFERENCES client(client_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Execute schema creation SQL\n",
    "        for result in cursor.execute(schema_sql, multi=True):\n",
    "            if result.with_rows:\n",
    "                print(f\"Rows returned: {result.fetchall()}\")\n",
    "            else:\n",
    "                print(f\"Affected {result.rowcount} rows.\")\n",
    "        print(\"Schema created successfully.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error creating schema: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Use existing connection\n",
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    try:\n",
    "        create_schema(conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/312344124.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  existing_makes = pd.read_sql(\"SELECT make_name FROM make\", conn)['make_name'].tolist()\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/312344124.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  make_table = pd.read_sql(\"SELECT * FROM make\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missing makes: ['Nissan', 'Ford', 'Honda', 'Toyota', 'Chevrolet', 'Maruti', 'Hyundai', 'Datsun', 'Tata', 'Jaguar', 'Mercedes-Benz', 'Audi', 'Skoda', 'Jeep', 'BMW', 'Mahindra', 'Renault', 'Fiat', 'Volkswagen', 'Volvo', 'Mitsubishi', 'Land', 'Daewoo', 'MG', 'Force', 'Isuzu', 'OpelCorsa', 'Ambassador', 'Kia', 'Lexus', 'INFINITI', 'Acura', 'Tesla', 'Aston', 'Lincoln', 'Dodge', 'Genesis', 'Bentley', 'Lucid', 'MINI', 'Porsche', 'Hummer', 'Chrysler', 'Cadillac', 'Lamborghini', 'Maserati', 'Subaru', 'Rivian', 'GMC', 'RAM', 'Alfa', 'Ferrari', 'Scion', 'Mazda', 'Saturn', 'Polestar', 'Rolls-Royce', 'McLaren', 'Buick', 'Lotus', 'Pontiac', 'FIAT', 'Karma', 'Saab', 'Mercury', 'Plymouth', 'smart', 'Maybach', 'Suzuki']\n",
      "Added 69 missing makes.\n",
      "Data inserted into `vehicle` table successfully!\n"
     ]
    }
   ],
   "source": [
    "def handle_missing_makes(vehicle_data, conn):\n",
    "    \"\"\"\n",
    "    Handles missing make_name entries by adding them to the make table in bulk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract all unique makes from the vehicle data\n",
    "        unique_makes = vehicle_data['make_name'].dropna().unique()\n",
    "\n",
    "        # Fetch existing makes from the database\n",
    "        existing_makes = pd.read_sql(\"SELECT make_name FROM make\", conn)['make_name'].tolist()\n",
    "\n",
    "        # Identify missing makes\n",
    "        missing_makes = [make for make in unique_makes if make not in existing_makes]\n",
    "\n",
    "        if missing_makes:\n",
    "            print(f\"Adding missing makes: {missing_makes}\")\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            insert_query = \"INSERT INTO make (make_name) VALUES (%s)\"\n",
    "            \n",
    "            # Insert each missing make\n",
    "            cursor.executemany(insert_query, [(make,) for make in missing_makes])\n",
    "            conn.commit()\n",
    "            cursor.close()\n",
    "            print(f\"Added {len(missing_makes)} missing makes.\")\n",
    "        else:\n",
    "            print(\"No missing makes to add.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error in handle_missing_makes: {e}\")\n",
    "\n",
    "def prepare_vehicle_data(vehicle_data, conn, row_limit=20000):\n",
    "    \"\"\"\n",
    "    Prepares the vehicle data and resolves missing make_name issues.\n",
    "    Limits the processed data to the specified row limit.\n",
    "    \"\"\"\n",
    "    # Step 1: Handle missing makes (process all makes, regardless of row limit)\n",
    "    handle_missing_makes(vehicle_data, conn)\n",
    "\n",
    "    # Step 2: Reload the updated make table\n",
    "    make_table = pd.read_sql(\"SELECT * FROM make\", conn)\n",
    "\n",
    "    # Step 3: Merge make_id into vehicle_data\n",
    "    vehicle_data = vehicle_data.merge(make_table, on='make_name', how='left')\n",
    "\n",
    "    # Step 4: Filter out rows with missing make_id\n",
    "    vehicle_data = vehicle_data[vehicle_data['make_id'].notna()]\n",
    "\n",
    "    # Step 5: Reapply row limit\n",
    "    vehicle_data = vehicle_data.head(row_limit)\n",
    "\n",
    "    # Step 6: Truncate long model values to match schema constraints\n",
    "    vehicle_data['model'] = vehicle_data['model'].str.slice(0, 50)\n",
    "\n",
    "    # Step 7: Handle missing or invalid `commission_rate` values\n",
    "    vehicle_data['commission_rate'] = vehicle_data['commission_rate'].fillna(0.0)  # Replace NaN with 0.0\n",
    "\n",
    "    return vehicle_data[['make_id', 'model', 'year', 'price', 'commission_rate']]\n",
    "\n",
    "\n",
    "# Main script\n",
    "conn = connect_to_db()\n",
    "\n",
    "if conn:\n",
    "    try:\n",
    "        # Combine all datasets into a single DataFrame\n",
    "        car_dekho_vehicles = car_dekho_data[['name', 'year', 'price']].rename(columns={\n",
    "            'name': 'model', 'year': 'year', 'price': 'price'\n",
    "        })\n",
    "        car_dekho_vehicles['make_name'] = car_dekho_vehicles['model'].str.split().str[0]\n",
    "        car_dekho_vehicles['commission_rate'] = None  # No commission rate in this dataset\n",
    "\n",
    "        used_cars_vehicles = used_cars_data[['brand', 'model', 'model_year', 'price']].rename(columns={\n",
    "            'brand': 'make_name', 'model': 'model', 'model_year': 'year', 'price': 'price'\n",
    "        })\n",
    "        used_cars_vehicles['commission_rate'] = None  # No commission rate in this dataset\n",
    "\n",
    "        sales_vehicles = clean_car_sales_data[['Car Make', 'Car Model', 'Car Year', 'Sale Price', 'Commission Rate']].rename(columns={\n",
    "            'Car Make': 'make_name', 'Car Model': 'model', 'Car Year': 'year', 'Sale Price': 'price', 'Commission Rate': 'commission_rate'\n",
    "        })\n",
    "\n",
    "        # Combine all datasets\n",
    "        combined_vehicles = pd.concat([sales_vehicles, car_dekho_vehicles, used_cars_vehicles], ignore_index=True)\n",
    "\n",
    "        # Prepare and clean vehicle data with row limit applied\n",
    "        vehicle_data = prepare_vehicle_data(combined_vehicles, conn, row_limit=20000)\n",
    "\n",
    "        # Populate the vehicle table\n",
    "        populate_vehicle_table(vehicle_data, conn)\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data:\n",
      "Connection successful!\n",
      "Data inserted successfully into the `store` table!\n"
     ]
    }
   ],
   "source": [
    "store_data = enterprise_station_df[\n",
    "    ['tid', 'brand', 'loc_name', 'loc_number', 'loc_type', 'address_1',\n",
    "     'country', 'city', 'state', 'postal_code', 'latitude', 'longitude']\n",
    "].rename(columns={\n",
    "    'tid': 'store_id',\n",
    "    'loc_name': 'location_name',\n",
    "    'loc_number': 'location_number',\n",
    "    'loc_type': 'location_type',\n",
    "    'address_1': 'address'\n",
    "}).replace({np.nan: None})\n",
    "\n",
    "print(\"Prepared data:\")\n",
    "\n",
    "conn = connect_to_db()\n",
    "\n",
    "if conn:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO store (\n",
    "            store_id, brand, location_name, location_number, location_type, \n",
    "            address, country, city, state, postal_code, latitude, longitude\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        data_tuples = list(store_data.itertuples(index=False, name=None))\n",
    "        cursor.executemany(insert_query, data_tuples)\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully into the `store` table!\")\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate client, staff, sale, & commissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Data inserted into `staff` table successfully! 9354 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3077918814.py:38: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  store_ids = pd.read_sql(\"SELECT store_id FROM store\", conn)['store_id'].tolist()\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3077918814.py:81: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  client_ids = pd.read_sql(\"SELECT client_id, name FROM client\", conn)\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3077918814.py:82: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  staff_ids = pd.read_sql(\"SELECT staff_id, name AS staff_name FROM staff\", conn)\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3077918814.py:83: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  make_ids = pd.read_sql(\"SELECT make_id, make_name FROM make\", conn)\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3077918814.py:84: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  vehicle_ids = pd.read_sql(\"SELECT vehicle_id, make_id FROM vehicle\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 records (Batch 1).\n",
      "Inserted 1000 records (Batch 2).\n",
      "Inserted 1000 records (Batch 3).\n",
      "Inserted 1000 records (Batch 4).\n",
      "Inserted 1000 records (Batch 5).\n",
      "Inserted 1000 records (Batch 6).\n",
      "Inserted 1000 records (Batch 7).\n",
      "Inserted 1000 records (Batch 8).\n",
      "Inserted 1000 records (Batch 9).\n",
      "Inserted 1000 records (Batch 10).\n",
      "Data inserted into `commission` table successfully! 10000 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3077918814.py:126: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sale_data = pd.read_sql(f\"\"\"\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 10000\n",
    "def prepare_client_data(sales_data, conn, batch_size=1000, row_limit=LIMIT):\n",
    "    cursor = None\n",
    "    try:\n",
    "        sales_data = sales_data.iloc[:row_limit]\n",
    "        \n",
    "        client_data = sales_data[['Customer Name', 'Sale Price']].drop_duplicates()\n",
    "        client_data = client_data.rename(columns={'Customer Name': 'name'})\n",
    "        client_data['age'] = None \n",
    "        client_data['salary'] = client_data['Sale Price'] * 3.5\n",
    "        client_data['purchased'] = 1 \n",
    "        client_data = client_data[['name', 'salary', 'purchased']]\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO client (name, salary, purchased)\n",
    "        VALUES (%s, %s, %s)\n",
    "        \"\"\"\n",
    "        for i in range(0, len(client_data), batch_size):\n",
    "            batch = client_data.iloc[i:i+batch_size].to_records(index=False).tolist()\n",
    "            cursor.executemany(insert_query, batch)\n",
    "            conn.commit()\n",
    "    except Error as e:\n",
    "        print(f\"Error preparing client data: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "\n",
    "\n",
    "def prepare_staff_data(sales_data, conn, row_limit=LIMIT):\n",
    "    cursor = None\n",
    "    try:\n",
    "        sales_data = sales_data.iloc[:row_limit]\n",
    "\n",
    "        staff_data = sales_data[['Salesperson']].drop_duplicates()\n",
    "        staff_data = staff_data.rename(columns={'Salesperson': 'name'})\n",
    "\n",
    "        store_ids = pd.read_sql(\"SELECT store_id FROM store\", conn)['store_id'].tolist()\n",
    "        if not store_ids:\n",
    "            raise ValueError(\"No store_ids found in the `store` table.\")\n",
    "\n",
    "        staff_data['store_id'] = [random.choice(store_ids) for _ in range(len(staff_data))]\n",
    "\n",
    "        staff_data['hire_date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO staff (name, store_id, hire_date)\n",
    "        VALUES (%s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(insert_query, staff_data.to_records(index=False).tolist())\n",
    "        conn.commit()\n",
    "        print(f\"Data inserted into `staff` table successfully! {len(staff_data)} records.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error preparing staff data: {e}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error: {ve}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "\n",
    "\n",
    "def prepare_sale_data(sales_data, conn, batch_size=1000, row_limit=LIMIT):\n",
    "    \"\"\"\n",
    "    Prepares and populates the `sale` table, including the commission_rate column.\n",
    "    \"\"\"\n",
    "    cursor = None\n",
    "    try:\n",
    "        sales_data = sales_data.iloc[:row_limit]\n",
    "\n",
    "        sale_data = sales_data[['Customer Name', 'Salesperson', 'Car Make', 'Sale Price', 'Commission Rate']].rename(\n",
    "            columns={\n",
    "                'Customer Name': 'name',\n",
    "                'Salesperson': 'staff_name',\n",
    "                'Car Make': 'make_name',\n",
    "                'Sale Price': 'sale_price',\n",
    "                'Commission Rate': 'commission_rate'\n",
    "            }\n",
    "        ).drop_duplicates()\n",
    "\n",
    "        client_ids = pd.read_sql(\"SELECT client_id, name FROM client\", conn)\n",
    "        staff_ids = pd.read_sql(\"SELECT staff_id, name AS staff_name FROM staff\", conn)\n",
    "        make_ids = pd.read_sql(\"SELECT make_id, make_name FROM make\", conn)\n",
    "        vehicle_ids = pd.read_sql(\"SELECT vehicle_id, make_id FROM vehicle\", conn)\n",
    "\n",
    "        sale_data = sale_data.merge(client_ids, on='name', how='left')\n",
    "        sale_data = sale_data.merge(staff_ids, on='staff_name', how='left')\n",
    "        sale_data = sale_data.merge(make_ids, on='make_name', how='left')\n",
    "        sale_data = sale_data.merge(vehicle_ids, on='make_id', how='left')\n",
    "\n",
    "        sale_data = sale_data.dropna(subset=['vehicle_id', 'client_id', 'staff_id'])\n",
    "\n",
    "        sale_data = sale_data.head(row_limit)\n",
    "\n",
    "        sale_data = sale_data[['vehicle_id', 'client_id', 'staff_id', 'sale_price', 'commission_rate']]\n",
    "        sale_data = sale_data.astype({\n",
    "            'vehicle_id': 'int',\n",
    "            'client_id': 'int',\n",
    "            'staff_id': 'int',\n",
    "            'commission_rate': 'float'\n",
    "        })\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO sale (vehicle_id, client_id, staff_id, sale_price, commission_rate, sale_date)\n",
    "        VALUES (%s, %s, %s, %s, %s, CURDATE())\n",
    "        \"\"\"\n",
    "        for start in range(0, len(sale_data), batch_size):\n",
    "            batch = sale_data.iloc[start:start + batch_size].to_records(index=False).tolist()\n",
    "            cursor.executemany(insert_query, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(batch)} records (Batch {start // batch_size + 1}).\")\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error preparing sale data: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "\n",
    "def prepare_commission_data(conn, row_limit=LIMIT):\n",
    "    \"\"\"\n",
    "    Prepares and populates the `commission` table using sale data.\n",
    "    \"\"\"\n",
    "    cursor = None\n",
    "    try:\n",
    "        sale_data = pd.read_sql(f\"\"\"\n",
    "            SELECT sale_id, staff_id, sale_price, commission_rate \n",
    "            FROM sale \n",
    "            LIMIT {row_limit}\n",
    "        \"\"\", conn)\n",
    "\n",
    "        sale_data['amount'] = sale_data['sale_price'] * sale_data['commission_rate']\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO commission (sale_id, staff_id, amount)\n",
    "        VALUES (%s, %s, %s)\n",
    "        \"\"\"\n",
    "        commission_records = sale_data[['sale_id', 'staff_id', 'amount']].to_records(index=False).tolist()\n",
    "\n",
    "        cursor.executemany(insert_query, commission_records)\n",
    "        conn.commit()\n",
    "        print(f\"Data inserted into `commission` table successfully! {len(commission_records)} records.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error preparing commission data: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "\n",
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    try:\n",
    "        prepare_client_data(sales_data, conn)\n",
    "        prepare_staff_data(sales_data, conn)\n",
    "        prepare_sale_data(sales_data, conn)\n",
    "        prepare_commission_data(conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate rental, vehicle_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3416151994.py:92: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  vehicle_map = pd.read_sql(query, conn).set_index('model')['vehicle_id'].to_dict()\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3416151994.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  client_map = pd.read_sql(query, conn).set_index('name')['client_id'].to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Rental data columns: Index(['airport', 'airport_iata', 'country', 'city', 'rental_length',\n",
      "       'start_date', 'start_time', 'return_date', 'return_time', 'date_offset',\n",
      "       'deposit_price', 'drive_away_price', 'price', 'currency',\n",
      "       'product_name', 'product_id', 'airbags', 'aircon', 'free_cancellation',\n",
      "       'doors', 'group', 'seats', 'transmission', 'mileage', 'supplier_name',\n",
      "       'supplier_address', 'supplier_loction_type', 'average', 'average_text',\n",
      "       'cleanliness', 'condition', 'dropoff_time', 'efficiency', 'location',\n",
      "       'pickup_time', 'value_for_money', 'no_of_ratings', 'RunDate',\n",
      "       'setup_prams', 'tid'],\n",
      "      dtype='object')\n",
      "Unmapped product_names: ['Fiat 500' 'Kia Rio' 'Kia Stonic' 'Seat Leon' 'Honda CR-V'\n",
      " 'Volkswagen Golf' 'Nissan Qashqai' 'Nissan Qashqai   ' 'Ford Mondeo'\n",
      " 'Kia Sportage' 'Vauxhall Corsa' 'Polestar 2' 'Vauxhall Crossland X'\n",
      " 'Peugeot 2008' 'Ford Fiesta' 'Ford Focus' 'Volkswagen Passat'\n",
      " 'Citroen C3 Aircross' 'Vauxhall Astra' 'Volvo XC40' 'Toyota Aygo'\n",
      " 'Hyundai Tucson' 'Tesla Model 3' 'Peugeot e-208 GPS' 'Peugeot 208'\n",
      " 'Volkswagen Polo' 'Audi Q3' 'Volkswagen T-Cross' 'Seat Alhambra'\n",
      " 'Seat Leon Estate' 'Mercedes C-Class' 'Peugeot 308' 'Audi A1'\n",
      " 'Mazda MX-30' 'Vauxhall Mokka-e' 'Audi Q5' 'Vauxhall Mokka' 'MG ZS'\n",
      " 'Audi A5' 'Kia e-Niro' 'Peugeot 3008' 'Skoda Karoq' 'Tesla Model Y'\n",
      " 'Mercedes GLE' 'Mercedes E-Class' 'Audi Q3   ' 'Volkswagen ID.3'\n",
      " 'Skoda Octavia' 'BMW 1 Series' 'Hyundai IONIQ' 'Nissan Leaf'\n",
      " 'Citroen C5 Aircross' 'BMW 2 Series' 'BMW 3 Series' 'Kia Ceed'\n",
      " 'Citroen C4' 'Mini Cooper' 'Mercedes A-Class' 'Renault Twingo'\n",
      " 'Renault Clio' 'Opel Insignia' 'Volvo XC60' 'Fiat 500e' 'Opel Corsa'\n",
      " 'Hyundai Kona' 'Opel Mokka' 'Peugeot e-208' 'Hyundai Kona GPS'\n",
      " 'Volkswagen ID.4' 'Nissan X-Trail' 'Peugeot 308 Estate' 'Opel Vivaro'\n",
      " 'Nissan X-Trail   ' 'Ford Kuga' 'Opel Corsa GPS' 'Citroen Seres 3 GPS'\n",
      " 'Renault Captur' 'Audi A1 Sportback' 'Fiat 500e Electric'\n",
      " 'Volkswagen Sharan' 'BMW X1' 'Opel Crossland' 'Toyota Yaris'\n",
      " 'Dacia Sandero' 'Volkswagen T-Roc' 'Toyota ProAce' 'Renault Arkana'\n",
      " 'Renault Trafic' 'Toyota Aygo X' 'Toyota C-HR GPS' 'Peugeot e-2008'\n",
      " 'Citroen DS7 Hybrid' 'Renault  Megane E-Tech' 'Peugeot 508'\n",
      " 'Toyota  Yaris Cross' 'Audi Q2' 'Peugeot 208   ' 'Renault Grand Scenic'\n",
      " 'MG EHS' 'Mazda 2' 'Volvo  XC40' 'Peugeot 5008' 'Mercedes CLA'\n",
      " 'Citroen DS7' 'Audi A6 Avant Estate' 'BMW X5 4x4' 'BMW X3' 'BMW 4 Series'\n",
      " 'Mazda  CX-60' 'Renault Megane Estate' 'Toyota Corolla Estate'\n",
      " 'Citroen C3' 'Opel Corsa   ' 'Suzuki Vitara' 'Mitsubishi Space Star'\n",
      " 'Ford Fiesta  ' 'Mazda 6' 'Kia Picanto' 'Mazda CX-3' 'Fiat Panda'\n",
      " 'Volvo V40' 'Mazda 3' 'Jaguar I-Pace' 'Toyota C-HR' 'Opel Astra Estate'\n",
      " 'Nissan Micra' 'Kia Niro' 'Volvo S60' 'Lynk & Co 01' 'Kia Venga'\n",
      " 'Kia Ceed Estate' 'Audi A3' 'Audi A4' 'Audi A4 Estate' 'BMW iX'\n",
      " 'Volvo V90 Estate' 'Volvo S90' 'Audi A6' 'Skoda Octavia Estate'\n",
      " 'Hyundai I40 Estate' 'Seat Leon    Estate' 'Mazda 6 Estate'\n",
      " 'Opel Insignia Estate' 'Volvo V60 Estate' 'Citroen DS3' 'Skoda Fabia'\n",
      " 'Skoda Scala' 'Hyundai i30' 'Skoda Kamiq' 'Fiat 500 Cabrio'\n",
      " 'Skoda Fabia Estate GPS' 'Citroen C5' 'Mercedes E-Class GPS' 'Opel Adam'\n",
      " 'Opel Astra' 'Volkswagen Golf GPS' 'Skoda Octavia GPS' 'Ford Focus GPS'\n",
      " 'Volkswagen Tiguan' 'Mercedes C-Class GPS' 'Opel Crossland X'\n",
      " 'Citroen e-C4' 'Audi e-tron' 'Volkswagen Touareg 4x4 GPS'\n",
      " 'Mercedes C-Class Estate' 'Opel  Adam' 'Ford Kuga GPS' 'Seat Ateca'\n",
      " 'DS 3 Crossback E-Tense' 'Hyundai i20' 'Volkswagen Touareg' 'Seat Ibiza'\n",
      " 'BMW X5' 'Volkswagen Tiguan Allspace' 'Citroen C5 X' 'Opel Grandland X'\n",
      " 'Seat Arona' 'Audi A8   ' 'BMW Z4 Cabrio' 'Hyundai i30 Estate'\n",
      " 'Opel Zafira GPS' 'Mazda MX-5 Cabrio' 'Seat Leon GPS'\n",
      " 'Volkswagen Polo   ' 'Audi A6   ' 'Audi A5 Sportback'\n",
      " 'Mercedes C-Class  Estate' 'Audi A6 Estate' 'Audi A6  Estate'\n",
      " 'Ford Focus Estate' 'BMW X2' 'Volkswagen up!' 'Audi A3   '\n",
      " 'Mini Cooper Cabrio' 'Audi A7' 'Audi A4 Avant Estate'\n",
      " 'Peugeot 308 Estate GPS' 'Renault Zoe' 'Hyundai Tucson   '\n",
      " 'Alfa Romeo Stelvio' 'Toyota RAV4' 'Alfa Romeo Giulietta' 'Opel Astra   '\n",
      " 'Mercedes V-Class' 'Mini Countryman' 'Mercedes Vito' 'Renault Megane'\n",
      " 'Seat Toledo' 'Renault Kadjar' 'Fiat Tipo Estate' 'Toyota RAV4 GPS'\n",
      " 'Toyota Corolla  ' 'Jaguar F-Pace' 'Ford Transit' 'Peugeot 308   '\n",
      " 'Renault Espace' 'Volkswagen Caddy' 'Audi A3 Sportback' 'Toyota Prius'\n",
      " 'Renault Scenic' 'Toyota Corolla' 'Toyota C-HR Hybrid' 'Audi  Q3'\n",
      " 'Fiat 500L' 'Citroen C-Elysee' 'Toyota Corolla Hybrid' 'Peugeot 108'\n",
      " 'Volkswagen Touran' 'Fiat Tipo' 'Citroen C1' 'Mercedes  E-Class  '\n",
      " 'BMW 5 Series' 'BMW 7 Series' 'Citroen C4 Grand Picasso'\n",
      " 'Hyundai Ioniq Hybrid' 'Volkswagen Golf   ' 'Renault Clio Estate'\n",
      " 'Hyundai  i30' 'Ford Kuga Hybrid' 'Alfa Romeo Giulia'\n",
      " 'Volkswagen Caravelle   ' 'Volkswagen Caravelle' 'Fiat Talento' 'BMW i3'\n",
      " 'Cupra Formentor' 'BMW 4 Series Cabrio' 'BMW X2 Hybrid' 'Fiat Egea'\n",
      " 'Renault Symbol Thalia' 'Peugeot 301' 'Hyundai Bayon' 'Nissan Juke'\n",
      " 'Dacia Duster' 'Volkswagen Taigo' 'Citroen C4-Elysee' 'Fiat Tipo Aegea'\n",
      " 'Renault Taliant' 'Hyundai I20' 'Dacia Lodgy' 'Ford Tourneo'\n",
      " 'Citroen Nemo' 'Fiat Doblo' 'Fiat Egea Cross' 'Skoda Superb'\n",
      " 'Peugeot Traveller' 'Volkswagen Transporter' 'Hyundai i10' 'BMW 5-series'\n",
      " 'Renault Megane Sedan' 'Renault Symbol' 'Hyundai Accent' 'Jeep Renegade'\n",
      " 'BMW iX3' 'Jeep Compass' 'Alfa Romeo Tonale' 'Opel  Astra'\n",
      " 'Porsche Macan' 'Audi A3 Saloon - Sedan' 'Volvo XC90' 'Audi Q3 Sportback'\n",
      " 'Volkswagen Passat Estate' 'Skoda Superb Estate' 'Volkswagen Passat   '\n",
      " 'Audi Q5 GPS' 'Mercedes E-Class Estate' 'Citroen C4 Grand Picasso GPS'\n",
      " 'BYD Atto 3' 'BMW iX xDrive40' 'Ford Puma' 'Opel Astra GPS' 'Ford Focus '\n",
      " 'Ford Focus   ' 'Citroen C1 GPS' 'Ford Ka' 'Hyundai Tucson GPS'\n",
      " 'Hyundai ix35' 'Toyota Avensis' 'Opel Zafira' 'Volkswagen Jetta'\n",
      " 'Skoda Citigo' 'Dacia Jogger' 'Toyota Avensis GPS' 'Volvo S90 GPS'\n",
      " 'Volkswagen Jetta   ' 'Skoda Kodiaq' 'Hyundai Santa Fe'\n",
      " 'Renault Grand Megane' 'Volkswagen Golf Estate' 'BMW 3 Series Estate'\n",
      " 'Fiat 500X 4x4' 'Seat Ateca 4x4' 'Opel Mokka AWD' 'Seat  Ibiza'\n",
      " 'Volkswagen Tiguan 4x4' 'Ford Kuga 4x4' 'Volkswagen ID.Buzz'\n",
      " 'Audi A5 Cabrio' 'Mercedes A-Class GPS' 'Audi Q5 AWD GPS' 'Kia Sorento'\n",
      " 'Volkswagen Golf Variant Estate' 'BMW 5 Series GPS' 'Toyota Highlander'\n",
      " 'Audi A6 Estate AWD' 'Volkswagen Passat 4x4 GPS' 'Volkswagen Touareg AWD'\n",
      " 'Audi A4 4x4 GPS' 'Audi A4 Estate AWD' 'Audi A5 4x4 GPS'\n",
      " 'Ford Mondeo Estate' 'Volvo V60 Estate AWD GPS'\n",
      " 'Mercedes E-Class Estate GPS' 'Audi Q8 4x4 GPS'\n",
      " 'Mercedes CLA Shooting Brake' 'Mercedes GLE GPS' 'Audi Q7'\n",
      " 'Mercedes S-Class GPS' 'Volkswagen Up!   ' 'SEAT Ateca'\n",
      " 'Renault Twingo   ' 'Ford Focus  ' 'Toyota Proace' 'Citroen C4 Cactus'\n",
      " 'Hyundai Kauai' 'Fiat Punto' 'Mitsubishi Eclipse Cross' 'Peugeot 3008   '\n",
      " 'BMW 318 Touring' 'Mercedes GLC' 'Mercedes B-Class' 'Mercedes EQC'\n",
      " 'Mercedes C-Class   ' 'Range Rover Evoque' 'Fiat  500 Cabrio'\n",
      " 'BMW 5 Series Estate' 'Mercedes E-Class Cabrio' 'Volkswagen ID.3 GPS'\n",
      " 'BMW X4' 'Tesla Model Y 4x4 GPS' 'Mercedes GLA' 'BMW 2 Series Cabrio'\n",
      " 'Citroen DS4' 'Mini One Cabrio' 'Mercedes CLA Estate'\n",
      " 'Land Rover Discovery' 'Mini  Cooper' 'Volkswagen Golf  Estate'\n",
      " 'Renault Clio    Estate' 'Mini One' 'Seat Tarraco' 'SEAT Arona'\n",
      " 'Cupra Born' 'Peugeot E-208' 'Fiat 500X' 'Lancia Ypsilon' 'Smart Forfour'\n",
      " 'Opel Grandland' 'Fiat 500 Hybrid ' 'Citroen C4 Grand Spacetourer'\n",
      " 'Aiways U5' 'Volkswagen Passat Estate GPS' 'Audi A4 Estate GPS'\n",
      " 'Audi A4  Estate GPS' 'BMW 2 Series Coupe' 'Jeep Compass 4xe'\n",
      " 'BMW 4 Series Coupe' 'Cupra Formentor 4x4' 'Citroen C4 Grand Picasso '\n",
      " 'Nissan Townstar' 'Citroen C4 Grand SpaceTourer' 'Ford Ecosport'\n",
      " 'Maserati Levante' 'Citroen DS7 Crossback' 'Peugeot 308 GPS'\n",
      " 'Toyota Yaris Hybrid' 'Skoda Octavia    Estate' 'Cadillac XT4'\n",
      " 'Opel Zafira   ' 'Alfa Romeo Giulia GPS' 'Ford Focus    Estate'\n",
      " 'Opel Mokka-e' 'Toyota Auris' 'Lexus UX' 'Citroen Berlingo'\n",
      " 'BMW 520 Estate GPS']\n",
      "Unmapped airports: ['Heathrow Airport' 'Charles de Gaulle Airport'\n",
      " 'Amsterdam Airport Schiphol' 'Frankfurt am Main Airport'\n",
      " 'Adolfo Surez MadridBarajas Airport'\n",
      " 'Josep Tarradellas BarcelonaEl Prat Airport' 'Istanbul Airport'\n",
      " 'Munich Airport' 'Dublin Airport' 'Orly Airport' 'Zurich Airport'\n",
      " 'Lisbon Airport' 'Copenhagen Airport' 'Malpensa Airport']\n",
      "Number of rental records to insert: 2730\n",
      "Data inserted into `rental` table successfully! 2730 rows.\n",
      "Rows in `rental` table after insertion: 2730\n",
      "Missing vehicle_id mappings: 2730 replaced with random IDs.\n",
      "Number of performance records to insert: 2571\n",
      "Data inserted into `vehicle_performance` table successfully! 2571 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3416151994.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  vehicle_ids = pd.read_sql(\"SELECT vehicle_id FROM vehicle\", conn)['vehicle_id'].tolist()\n",
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/3416151994.py:92: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  vehicle_map = pd.read_sql(query, conn).set_index('model')['vehicle_id'].to_dict()\n"
     ]
    }
   ],
   "source": [
    "def populate_vehicle_performance_table(rental_data, conn):\n",
    "    \"\"\"\n",
    "    Populate the vehicle_performance table based on rental reviews.\n",
    "    If a vehicle_id is missing, assign a random vehicle_id from the vehicle table.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        vehicle_ids = pd.read_sql(\"SELECT vehicle_id FROM vehicle\", conn)['vehicle_id'].tolist()\n",
    "\n",
    "        vehicle_map = get_vehicle_id_from_db(conn)\n",
    "        rental_data['vehicle_id'] = rental_data['product_name'].str.strip().str.lower().map(vehicle_map)\n",
    "\n",
    "        missing_vehicle_ids = rental_data['vehicle_id'].isna().sum()\n",
    "        rental_data['vehicle_id'] = rental_data['vehicle_id'].apply(\n",
    "            lambda x: x if pd.notna(x) else random.choice(vehicle_ids)\n",
    "        )\n",
    "        print(f\"Missing vehicle_id mappings: {missing_vehicle_ids} replaced with random IDs.\")\n",
    "\n",
    "        performance_data = rental_data.groupby('vehicle_id').agg(\n",
    "            rating=('average', 'mean'),\n",
    "            trips_taken=('rental_length', 'sum'),\n",
    "            review_count=('no_of_ratings', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        performance_data = performance_data.fillna({\n",
    "            'rating': 0.0,\n",
    "            'trips_taken': 0,\n",
    "            'review_count': 0\n",
    "        })\n",
    "\n",
    "        performance_records = performance_data[['vehicle_id', 'rating', 'trips_taken', 'review_count']].values.tolist()\n",
    "\n",
    "        print(f\"Number of performance records to insert: {len(performance_records)}\")\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO vehicle_performance (vehicle_id, rating, trips_taken, review_count)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(insert_query, performance_records)\n",
    "        conn.commit()\n",
    "        print(f\"Data inserted into `vehicle_performance` table successfully! {len(performance_records)} rows.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting into vehicle_performance table: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "def populate_rental_table(rental_data, conn):\n",
    "    \"\"\"\n",
    "    Populate the rental table from the rental data.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        vehicle_map = get_vehicle_id_from_db(conn)\n",
    "        client_map = get_client_id_from_db(conn)\n",
    "\n",
    "        vehicle_map = {k.strip().lower(): v for k, v in vehicle_map.items()}\n",
    "        client_map = {k.strip().lower(): v for k, v in client_map.items()}\n",
    "\n",
    "        rental_data['vehicle_id'] = rental_data['product_name'].str.strip().str.lower().map(vehicle_map)\n",
    "        rental_data['client_id'] = rental_data['airport'].str.strip().str.lower().map(client_map)\n",
    "\n",
    "        unmapped_products = rental_data[rental_data['vehicle_id'].isna()]['product_name'].unique()\n",
    "        unmapped_airports = rental_data[rental_data['client_id'].isna()]['airport'].unique()\n",
    "        print(f\"Unmapped product_names: {unmapped_products}\")\n",
    "        print(f\"Unmapped airports: {unmapped_airports}\")\n",
    "\n",
    "        rental_data['vehicle_id'] = rental_data['vehicle_id'].fillna(random.choice(list(vehicle_map.values())))\n",
    "        rental_data['client_id'] = rental_data['client_id'].fillna(random.choice(list(client_map.values())))\n",
    "\n",
    "        rental_records = rental_data[[\n",
    "            'vehicle_id', 'client_id', 'price', 'rental_length',\n",
    "            'start_date', 'start_time', 'return_date', 'return_time'\n",
    "        ]].values.tolist()\n",
    "\n",
    "        print(f\"Number of rental records to insert: {len(rental_records)}\")\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO rental (vehicle_id, client_id, rental_rate, rental_length, start_date, start_time, return_date, return_time)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(insert_query, rental_records)\n",
    "        conn.commit()\n",
    "        print(f\"Data inserted into `rental` table successfully! {len(rental_records)} rows.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting into rental table: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def get_vehicle_id_from_db(conn):\n",
    "    query = \"SELECT vehicle_id, model FROM vehicle\"\n",
    "    vehicle_map = pd.read_sql(query, conn).set_index('model')['vehicle_id'].to_dict()\n",
    "    return vehicle_map\n",
    "\n",
    "def get_client_id_from_db(conn):\n",
    "    query = \"SELECT client_id, name FROM client\"\n",
    "    client_map = pd.read_sql(query, conn).set_index('name')['client_id'].to_dict()\n",
    "    return client_map\n",
    "\n",
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    try:\n",
    "        print(f\"Rental data columns: {rental_data.columns}\")\n",
    "\n",
    "        populate_rental_table(rental_data, conn)\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM rental\")\n",
    "        rental_count = cursor.fetchone()[0]\n",
    "        print(f\"Rows in `rental` table after insertion: {rental_count}\")\n",
    "        cursor.close()\n",
    "\n",
    "        populate_vehicle_performance_table(rental_data, conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate client_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Number of client accounts to insert: 10000\n",
      "Sample data: [(1, 3439.92, datetime.date(2022, 4, 18), 'Active'), (2, 4578.42, datetime.date(2022, 9, 24), 'Active'), (3, 2946.19, datetime.date(2022, 1, 19), 'Active'), (4, 4952.3, datetime.date(2022, 9, 6), 'Active'), (5, 2655.94, datetime.date(2023, 8, 17), 'Active')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/lp1b9ndj1kb78j95mcrtw_w80000gp/T/ipykernel_32441/2732936351.py:7: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  client_ids = pd.read_sql(\"SELECT client_id FROM client\", conn)['client_id'].tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into `client_account` table successfully! 10000 rows.\n"
     ]
    }
   ],
   "source": [
    "def populate_client_accounts(conn):\n",
    "    \"\"\"\n",
    "    Populate the client_account table with made-up data for all client IDs from the client table.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        client_ids = pd.read_sql(\"SELECT client_id FROM client\", conn)['client_id'].tolist()\n",
    "\n",
    "        client_accounts = []\n",
    "        for client_id in client_ids:\n",
    "            balance = round(random.uniform(0, 5000), 2)\n",
    "            created_date = datetime.now() - timedelta(days=random.randint(0, 365 * 5))\n",
    "            status = random.choice(['Active', 'Inactive', 'Suspended'])\n",
    "            client_accounts.append((client_id, balance, created_date.date(), status))\n",
    "\n",
    "        print(f\"Number of client accounts to insert: {len(client_accounts)}\")\n",
    "        print(f\"Sample data: {client_accounts[:5]}\")\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO client_account (client_id, balance, created_date, status)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(insert_query, client_accounts)\n",
    "        conn.commit()\n",
    "        print(f\"Data inserted into `client_account` table successfully! {len(client_accounts)} rows.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting into client_account table: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    try:\n",
    "        populate_client_accounts(conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all tables have been populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Table population check:\n",
      "Table `make`: 69 rows (Populated)\n",
      "Table `vehicle`: 20000 rows (Populated)\n",
      "Table `store`: 4177 rows (Populated)\n",
      "Table `staff`: 9354 rows (Populated)\n",
      "Table `client`: 10000 rows (Populated)\n",
      "Table `rental`: 2730 rows (Populated)\n",
      "Table `sale`: 10000 rows (Populated)\n",
      "Table `commission`: 10000 rows (Populated)\n",
      "Table `vehicle_performance`: 2571 rows (Populated)\n",
      "Table `client_account`: 10000 rows (Populated)\n",
      "\n",
      "All tables are populated!\n"
     ]
    }
   ],
   "source": [
    "def check_table_populations(conn):\n",
    "    tables = [\n",
    "        \"make\", \"vehicle\", \"store\", \"staff\", \"client\", \"rental\", \n",
    "        \"sale\", \"commission\", \"vehicle_performance\", \"client_account\"\n",
    "    ]\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"USE final;\")\n",
    "        \n",
    "        table_counts = {}\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table};\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            table_counts[table] = count\n",
    "        \n",
    "        print(\"Table population check:\")\n",
    "        for table, count in table_counts.items():\n",
    "            status = \"Populated\" if count > 0 else \"Empty\"\n",
    "            print(f\"Table `{table}`: {count} rows ({status})\")\n",
    "        \n",
    "        all_populated = all(count > 0 for count in table_counts.values())\n",
    "        if all_populated:\n",
    "            print(\"\\nAll tables are populated!\")\n",
    "        else:\n",
    "            print(\"\\nSome tables are empty. Check the data loading process.\")\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error checking table populations: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    try:\n",
    "        check_table_populations(conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"Failed to connect to the database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-vehicle-management-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
